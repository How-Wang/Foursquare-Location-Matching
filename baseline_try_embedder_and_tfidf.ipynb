{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfFhJlZbqTxJ"
      },
      "source": [
        "參考 link :https://www.kaggle.com/code/ryotayoshinobu/foursquare-lightgbm-baseline\n",
        "This notebook shows how to solve the problem as a multi-class classification by finding candidate points based on geographic location.<br>\n",
        "Similarity as a string, such as edit distance and LCS (Longest Common Subsequence), was used for the features of the candidate points.<br>\n",
        "<br>\n",
        "Inference is made on test data only, but the code for training is left commented out.<br>\n",
        "<br>\n",
        "In addition, making the matches bidirectional as a post-processing step improved the score by about 1%.<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OdbAnyhs5-J",
        "outputId": "a0d899d6-28fb-44df-e08a-d2cec8dfa0fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: lightgbm 2.2.3\n",
            "Uninstalling lightgbm-2.2.3:\n",
            "  Successfully uninstalled lightgbm-2.2.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lightgbm==3.3.1\n",
            "  Downloading lightgbm-3.3.1-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm==3.3.1) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0 in /usr/local/lib/python3.7/dist-packages (from lightgbm==3.3.1) (1.0.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm==3.3.1) (0.37.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightgbm==3.3.1) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.22.0->lightgbm==3.3.1) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.22.0->lightgbm==3.3.1) (1.1.0)\n",
            "Installing collected packages: lightgbm\n",
            "Successfully installed lightgbm-3.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 18.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 88.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (4.2.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 88.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 73.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=71459fb4c0df30cb4f6112c021a1c9a12f5c8278c820f9f1dc9c928e14ecbff7\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ],
      "source": [
        "! pip uninstall lightgbm -y\n",
        "! pip install lightgbm==3.3.1\n",
        "! pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHbnO4isqfj1",
        "outputId": "d6df6c98-3cf7-4445-9595-135e94d43403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.5.18.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Downloading foursquare-location-matching.zip to /content\n",
            " 91% 145M/159M [00:01<00:00, 107MB/s]\n",
            "100% 159M/159M [00:02<00:00, 83.1MB/s]\n",
            "Archive:  foursquare-location-matching.zip\n",
            "  inflating: pairs.csv               \n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c foursquare-location-matching\n",
        "! unzip foursquare-location-matching.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pouTaX_llCrb"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "H5QntWoelAkH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "from glob import glob\n",
        "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import sys\n",
        "from requests import get\n",
        "import multiprocessing\n",
        "import joblib\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# 宣告一個專門放參數的 class\n",
        "class CFG: # configuration\n",
        "    seed = 46\n",
        "    target = \"point_of_interest\"\n",
        "    n_neighbors = 10\n",
        "    n_splits = 3\n",
        "\n",
        "    expID = \"\"\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        expID = get(\"http://172.28.0.2:9000/api/sessions\").json()[0][\"name\"].split(\".\")[0]\n",
        "\n",
        "random.seed(CFG.seed)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(CFG.seed)\n",
        "np.random.seed(CFG.seed)\n",
        "\n",
        "plt.rcParams[\"font.size\"] = 13\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# %cd /content/drive/MyDrive/kaggle/foursquare-location-matching/{CFG.expID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "wz7JepVilAkN",
        "outputId": "7731a3f0-06a8-4165-c9e8-f8ae08d17b53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 id                     name   latitude   longitude  \\\n",
              "0  E_000001272c6c5d     Café Stad Oudenaarde  50.859975    3.634196   \n",
              "1  E_000002eae2a589           Carioca Manero -22.907225  -43.178244   \n",
              "2  E_000007f24ebc95         ร้านตัดผมการาเกด  13.780813  100.484900   \n",
              "3  E_000008a8ba4f48                 Turkcell  37.844510   27.844202   \n",
              "4  E_00001d92066153  Restaurante Casa Cofiño  43.338196   -4.326821   \n",
              "\n",
              "                  address        city            state   zip country  url  \\\n",
              "0             Abdijstraat  Nederename  Oost-Vlaanderen  9700      BE  NaN   \n",
              "1                     NaN         NaN              NaN   NaN      BR  NaN   \n",
              "2                     NaN         NaN              NaN   NaN      TH  NaN   \n",
              "3  Adnan Menderes Bulvarı         NaN              NaN   NaN      TR  NaN   \n",
              "4                     NaN    Caviedes        Cantabria   NaN      ES  NaN   \n",
              "\n",
              "  phone             categories point_of_interest  \n",
              "0   NaN                   Bars  P_677e840bb6fc7e  \n",
              "1   NaN  Brazilian Restaurants  P_d82910d8382a83  \n",
              "2   NaN   Salons / Barbershops  P_b1066599e78477  \n",
              "3   NaN     Mobile Phone Shops  P_b2ed86905a4cd3  \n",
              "4   NaN    Spanish Restaurants  P_809a884d4407fb  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bc018174-d760-4466-8ce7-498ce05edf83\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>address</th>\n",
              "      <th>city</th>\n",
              "      <th>state</th>\n",
              "      <th>zip</th>\n",
              "      <th>country</th>\n",
              "      <th>url</th>\n",
              "      <th>phone</th>\n",
              "      <th>categories</th>\n",
              "      <th>point_of_interest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>E_000001272c6c5d</td>\n",
              "      <td>Café Stad Oudenaarde</td>\n",
              "      <td>50.859975</td>\n",
              "      <td>3.634196</td>\n",
              "      <td>Abdijstraat</td>\n",
              "      <td>Nederename</td>\n",
              "      <td>Oost-Vlaanderen</td>\n",
              "      <td>9700</td>\n",
              "      <td>BE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bars</td>\n",
              "      <td>P_677e840bb6fc7e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>E_000002eae2a589</td>\n",
              "      <td>Carioca Manero</td>\n",
              "      <td>-22.907225</td>\n",
              "      <td>-43.178244</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>BR</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Brazilian Restaurants</td>\n",
              "      <td>P_d82910d8382a83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>E_000007f24ebc95</td>\n",
              "      <td>ร้านตัดผมการาเกด</td>\n",
              "      <td>13.780813</td>\n",
              "      <td>100.484900</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TH</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Salons / Barbershops</td>\n",
              "      <td>P_b1066599e78477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>E_000008a8ba4f48</td>\n",
              "      <td>Turkcell</td>\n",
              "      <td>37.844510</td>\n",
              "      <td>27.844202</td>\n",
              "      <td>Adnan Menderes Bulvarı</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TR</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Mobile Phone Shops</td>\n",
              "      <td>P_b2ed86905a4cd3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>E_00001d92066153</td>\n",
              "      <td>Restaurante Casa Cofiño</td>\n",
              "      <td>43.338196</td>\n",
              "      <td>-4.326821</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caviedes</td>\n",
              "      <td>Cantabria</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ES</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Spanish Restaurants</td>\n",
              "      <td>P_809a884d4407fb</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc018174-d760-4466-8ce7-498ce05edf83')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bc018174-d760-4466-8ce7-498ce05edf83 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bc018174-d760-4466-8ce7-498ce05edf83');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "train = pd.read_csv(\"./train.csv\")\n",
        "test = pd.read_csv(\"./test.csv\")\n",
        "test[CFG.target] = \"TEST\"\n",
        "train = train[:1000]\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO9c6tIe3j4B"
      },
      "source": [
        "# Devide Train Data into about 600K×2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6PcXKsn3pcK",
        "outputId": "4b6e174a-7927-4665-c33f-c6f24e3d11d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    500\n",
              "1.0    500\n",
              "Name: set, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "kf = GroupKFold(n_splits=2)\n",
        "for i, (trn_idx, val_idx) in enumerate(kf.split(train, train[CFG.target], train[CFG.target])):\n",
        "    train.loc[val_idx, \"set\"] = i\n",
        "train[\"set\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "Ewfee-Td3pYM",
        "outputId": "501ad056-1cb1-490a-db14-544dd8bc6238"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 id                     name   latitude   longitude  \\\n",
              "0  E_000001272c6c5d     Café Stad Oudenaarde  50.859975    3.634196   \n",
              "1  E_000002eae2a589           Carioca Manero -22.907225  -43.178244   \n",
              "2  E_000007f24ebc95         ร้านตัดผมการาเกด  13.780813  100.484900   \n",
              "3  E_000008a8ba4f48                 Turkcell  37.844510   27.844202   \n",
              "4  E_00001d92066153  Restaurante Casa Cofiño  43.338196   -4.326821   \n",
              "\n",
              "                  address        city            state   zip country  url  \\\n",
              "0             Abdijstraat  Nederename  Oost-Vlaanderen  9700      BE  NaN   \n",
              "1                     NaN         NaN              NaN   NaN      BR  NaN   \n",
              "2                     NaN         NaN              NaN   NaN      TH  NaN   \n",
              "3  Adnan Menderes Bulvarı         NaN              NaN   NaN      TR  NaN   \n",
              "4                     NaN    Caviedes        Cantabria   NaN      ES  NaN   \n",
              "\n",
              "  phone             categories point_of_interest  set  \n",
              "0   NaN                   Bars  P_677e840bb6fc7e  0.0  \n",
              "1   NaN  Brazilian Restaurants  P_d82910d8382a83  1.0  \n",
              "2   NaN   Salons / Barbershops  P_b1066599e78477  0.0  \n",
              "3   NaN     Mobile Phone Shops  P_b2ed86905a4cd3  1.0  \n",
              "4   NaN    Spanish Restaurants  P_809a884d4407fb  0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4a001a0a-9b6b-4f51-943c-292e0a613be6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>address</th>\n",
              "      <th>city</th>\n",
              "      <th>state</th>\n",
              "      <th>zip</th>\n",
              "      <th>country</th>\n",
              "      <th>url</th>\n",
              "      <th>phone</th>\n",
              "      <th>categories</th>\n",
              "      <th>point_of_interest</th>\n",
              "      <th>set</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>E_000001272c6c5d</td>\n",
              "      <td>Café Stad Oudenaarde</td>\n",
              "      <td>50.859975</td>\n",
              "      <td>3.634196</td>\n",
              "      <td>Abdijstraat</td>\n",
              "      <td>Nederename</td>\n",
              "      <td>Oost-Vlaanderen</td>\n",
              "      <td>9700</td>\n",
              "      <td>BE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bars</td>\n",
              "      <td>P_677e840bb6fc7e</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>E_000002eae2a589</td>\n",
              "      <td>Carioca Manero</td>\n",
              "      <td>-22.907225</td>\n",
              "      <td>-43.178244</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>BR</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Brazilian Restaurants</td>\n",
              "      <td>P_d82910d8382a83</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>E_000007f24ebc95</td>\n",
              "      <td>ร้านตัดผมการาเกด</td>\n",
              "      <td>13.780813</td>\n",
              "      <td>100.484900</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TH</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Salons / Barbershops</td>\n",
              "      <td>P_b1066599e78477</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>E_000008a8ba4f48</td>\n",
              "      <td>Turkcell</td>\n",
              "      <td>37.844510</td>\n",
              "      <td>27.844202</td>\n",
              "      <td>Adnan Menderes Bulvarı</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TR</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Mobile Phone Shops</td>\n",
              "      <td>P_b2ed86905a4cd3</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>E_00001d92066153</td>\n",
              "      <td>Restaurante Casa Cofiño</td>\n",
              "      <td>43.338196</td>\n",
              "      <td>-4.326821</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caviedes</td>\n",
              "      <td>Cantabria</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ES</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Spanish Restaurants</td>\n",
              "      <td>P_809a884d4407fb</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a001a0a-9b6b-4f51-943c-292e0a613be6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4a001a0a-9b6b-4f51-943c-292e0a613be6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4a001a0a-9b6b-4f51-943c-292e0a613be6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 測試 embedding & tfidf"
      ],
      "metadata": {
        "id": "zc0SrvssZlH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sentence_transformers import SentenceTransformer\n",
        "# model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# def make_embedding(input_columns):\n",
        "#     # embedding\n",
        "#     eb = model.encode(input_columns)\n",
        "#     # change 2d list to 1d string list (把其中的一個維度改成 string)\n",
        "#     tem_eb = [str(eb[i]) for i in range(len(eb))]\n",
        "\n",
        "#     return tem_eb"
      ],
      "metadata": {
        "id": "0uleZ_zqbTO0"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UClmfWIbYPGZ"
      },
      "outputs": [],
      "source": [
        "# name_embeddings = model.encode([train.iloc[1]['name'],train.iloc[0]['name']])\n",
        "# categories_embeddings = model.encode(train.iloc[1]['categories'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv9bCbMmbGtp"
      },
      "outputs": [],
      "source": [
        "# float(util.cos_sim(name_embeddings[0], name_embeddings[1])[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# def make_tfidf(input_columns):\n",
        "#   tv = TfidfVectorizer(use_idf=True, smooth_idf=True, norm=None)\n",
        "#   tv_fit = tv.fit_transform(input_columns)\n",
        "#   tv_fit = tv_fit.toarray().tolist()\n",
        "#   str_tv_fit = [str(tv_fit[i]) for i in range(len(tv_fit))]\n",
        "\n",
        "#   return str_tv_fit"
      ],
      "metadata": {
        "id": "gK-sJwQVPAJr"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result, tv = make_tfidf(list(train['name']))"
      ],
      "metadata": {
        "id": "GQso5cl-Q0jB"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(type(result[0]))"
      ],
      "metadata": {
        "id": "gHsQEr5qRpUp"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(tv.get_feature_names())"
      ],
      "metadata": {
        "id": "pKPM1NsYTvyS"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# import ast\n",
        "# # 這裡給兩個一維 dataframe, 可以跑出相同維度的分數\n",
        "# def cal_similarity(df_1, df_2):\n",
        "#     list_1 = list(df_1)\n",
        "#     list_2 = list(df_2)\n",
        "#     tf_1 = list(map(ast.literal_eval, list_1))\n",
        "#     tf_2 = list(map(ast.literal_eval, list_2))\n",
        "#     score_2d_list = cosine_similarity(tf_1, tf_1)\n",
        "\n",
        "#     score_1d_list = [score_2d_list[i][i] for i in range(len(score_2d_list))]\n",
        "#     return score_1d_list"
      ],
      "metadata": {
        "id": "Qr-TrpONTciq"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(result)"
      ],
      "metadata": {
        "id": "_NrPk6VERKeN"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yJIRkRD3jr-"
      },
      "source": [
        "# 資料代入 / 插補 / 填補 Data Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "rsyHcuGDlAkO"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "# 把每一個地點都加上 另外一個最近地點的資訊\n",
        "def add_neighbor_features(df):\n",
        "    dfs = []\n",
        "    # 需要 id 不然不沒辦法做後方 matches 的 set、沒辦法算分數\n",
        "    columns = ['id', 'name', 'address', 'city', 'state',\n",
        "           'zip', 'country', 'url', 'phone', 'categories']\n",
        "    # embedding_columns = ['name', 'categories']\n",
        "    # tfidf_columns = ['name', 'categories']\n",
        "\n",
        "    for c in columns:\n",
        "        if c != \"id\":\n",
        "            df[c] = df[c].astype(str).str.lower()\n",
        "            \n",
        "    # 把相同國家的 row 放進同一個 dataframe 裡面\n",
        "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
        "        country_df = country_df.reset_index(drop=True)\n",
        "        # n_neighbors 表示要參考附近的幾筆資料才決定屬於哪一群\n",
        "        k = min(len(country_df), CFG.n_neighbors)\n",
        "        knn = KNeighborsRegressor(n_neighbors=k, \n",
        "                                  metric='haversine', n_jobs=-1)\n",
        "        \n",
        "        # 使用相同的\"國家群\"進行訓練\n",
        "        # 同個國家的資料當成一群進行訓練\n",
        "        knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
        "        \n",
        "        # 得出分群後的資料，回傳每個點與最近的幾個點的距離與 indices\n",
        "        dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
        "        # 將 n_neighbors 各點的資料呈現在 country_df 上\n",
        "        targets = country_df[CFG.target].values\n",
        "        for i in range(k):\n",
        "            country_df[f\"d_near_{i}\"] = dists[:, i]\n",
        "            country_df[f\"near_target_{i}\"] = targets[nears[:, i]]\n",
        "            for c in columns:\n",
        "                country_df[f\"near_{c}_{i}\"] = country_df[c].values[nears[:, i]]\n",
        "            # for c in embedding_columns:\n",
        "                # country_df[f'near_embedding_{c}_{i}'] = make_embedding(country_df[c].values[nears[:, i]])\n",
        "        # 若整個國家的資料點數量小於預設所需的 k ，則將多出的部分填補為 nan\n",
        "        for i in range(k, CFG.n_neighbors):\n",
        "            country_df[f\"d_near_{i}\"] = np.nan\n",
        "            country_df[f\"near_target_{i}\"] = np.nan\n",
        "            for c in columns:\n",
        "                country_df[f\"near_{c}_{i}\"] = np.nan\n",
        "            # for c in embedding_columns:\n",
        "            #     country_df[f'near_embedding_{c}_{i}'] = np.nan\n",
        "\n",
        "        dfs.append(country_df)\n",
        "        # # for look inside\n",
        "        # df = pd.concat(dfs).reset_index(drop=True) # for look inside\n",
        "        # df.head()\n",
        "        # break\n",
        "    df = pd.concat(dfs).reset_index(drop=True)\n",
        "    # for c in tfidf_columns:\n",
        "    #     total_columns = []\n",
        "    #     for i in range(CFG.n_neighbors):\n",
        "    #         total_columns += list(df[f\"near_{c}_{i}\"].fillna('!'))\n",
        "    #     if c == 'name':\n",
        "    #         total_name_result =  make_tfidf(total_columns)\n",
        "    #         print(total_name_result)\n",
        "    #     elif c == 'categories':\n",
        "    #         total_cate_result = make_tfidf(total_columns)\n",
        "    #         print(total_cate_result)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "zVDxFZwqqTxV"
      },
      "outputs": [],
      "source": [
        "# original_train = train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "QpYTQiryqTxW"
      },
      "outputs": [],
      "source": [
        "# add_neighbor_features(original_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train"
      ],
      "metadata": {
        "id": "tAG76Ak0XebI"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzYSOaJDqTxW"
      },
      "outputs": [],
      "source": [
        "# 擴充 train 的 n_neighbors 的資料\n",
        "train = pd.concat([\n",
        "    add_neighbor_features(train[train[\"set\"]==0]), \n",
        "    add_neighbor_features(train[train[\"set\"]==1]), \n",
        "])\n",
        "\n",
        "test = add_neighbor_features(test)\n",
        "\n",
        "train.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNFoh2JT7rBy"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqqIg-885J3K"
      },
      "source": [
        "# Analysis: Threshold\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qurAOhRrqTxX"
      },
      "source": [
        "## 順位法\n",
        "以距離順位(k)為基準分析趨勢：\n",
        "1. 判斷多近「還有機會」是同個 POI\n",
        "    * 後面的結果會覆蓋前面的結果\n",
        "2. (?) 判斷多近「就會」是同個 POI\n",
        "    * p.s.「多近」是以距離順位（第幾近）來判斷，而非是以絕對距離來看。\n",
        "3. 判斷第一個 match 在哪 (0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftbdH97klAkQ"
      },
      "outputs": [],
      "source": [
        "train.loc[:,\"target\"] = 0\n",
        "# 1.\n",
        "# [x] 判斷前幾個相同分群的經緯度是相同 POI\n",
        "## 有可能第 2 個不同但第 4 個同，而 4 覆蓋掉 2，因此不代表前四個都是相同\n",
        "## [?] 但此處的問題為，把經緯度分群當成唯一影響，所以才直接照順序決定 target(同一個 POI 的情況下，最大的 i)\n",
        "\n",
        "# for i in range(CFG.n_neighbors):\n",
        "#     print(train[CFG.target]==train[f\"near_target_{i}\"])\n",
        "#     train.loc[train[CFG.target]==train[f\"near_target_{i}\"], \"target\"] = i\n",
        "\n",
        "# 2. (after 1.)\n",
        "# # wtf is that? >by Howard\n",
        "train.loc[:,\"not done\"] = True\n",
        "for i in tqdm(range(CFG.n_neighbors)):\n",
        "    train.loc[train[CFG.target]!=train[f\"near_target_{i}\"], \"not done\"] = False\n",
        "    train.loc[(train[CFG.target]==train[f\"near_target_{i}\"]) & (train[\"not done\"]), \"target\"] = i\n",
        "\n",
        "# 3.\n",
        "# for i in range(CFG.n_neighbors-1, -1, -1):\n",
        "# #     print(train[CFG.target]==train[f\"near_target_{i}\"])\n",
        "#     train.loc[train[CFG.target]==train[f\"near_target_{i}\"], \"target\"] = i\n",
        "    \n",
        "    \n",
        "# list(train.columns)\n",
        "train['target'].tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LUHHF3ynT-m"
      },
      "outputs": [],
      "source": [
        "train['target'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X97ptgolAkR"
      },
      "outputs": [],
      "source": [
        "plt.hist(train[\"target\"], bins=sorted(train[\"target\"].unique()))\n",
        "plt.grid()\n",
        "plt.xlabel(\"target\")\n",
        "plt.title('Number of matches until first unmatched')\n",
        "# plt.title('Number of the last match')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLh3aKWXqTxY"
      },
      "source": [
        "## 絕對距離法\n",
        "直接以距離的值來判定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wrXb5v-qTxY"
      },
      "outputs": [],
      "source": [
        "# for index, row in train.iterrows():\n",
        "# #     print(\"row\", row)\n",
        "# #     print(\"row['target']\", row[\"target\"])\n",
        "# #     print(\"row[row['target']]\", row[row[\"target\"]])\n",
        "# #     if index == 3:\n",
        "# #         break\n",
        "#     ## wtf is that? >by Howard\n",
        "#     row[\"max_dist\"] = row[row[\"target\"]]\n",
        "\n",
        "\n",
        "# for i in range(CFG.n_neighbors):\n",
        "#     train.loc[train[CFG.target]!=train[f\"near_target_{i}\"], \"done\"] = True\n",
        "#     ## Assign i to all elements in train['target'] where train['POI']==train[near_target_i]\n",
        "#     train.loc[(train[CFG.target]==train[f\"near_target_{i}\"]) & (train[\"done\"] != True), \"target\"] = train[f\"d_near_{i}\"]\n",
        "    \n",
        "# list(train.columns)\n",
        "# train['target'].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2a0xABiqTxY"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtM0grrKqTxY"
      },
      "source": [
        "## Two-way Hash\n",
        "用雜湊表來將正確答案產出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6wMZkeUqTxY"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/code/columbia2131/foursquare-iou-metrics\n",
        "\n",
        "## 將input dataframe 轉成 dict，再拿出 id 與 poi 用 dict 回傳\n",
        "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
        "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
        "\n",
        "## 將input dataframe 轉成 dict，再以 poi 為 index、取出\"屬於此 poi\"的 id set 用 dict 回傳\n",
        "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
        "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
        "\n",
        "id2poi = get_id2poi(train)\n",
        "poi2ids = get_poi2ids(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2CF7M8cqTxY"
      },
      "source": [
        "## 比對\n",
        "利用 KNN 的結果做出預測，再與正確答案做比對"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dDr4VvpqTxZ"
      },
      "outputs": [],
      "source": [
        "## 已有 matches\n",
        "def get_score(input_df: pd.DataFrame):\n",
        "    scores = []\n",
        "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
        "        # 找出與該 id 相同 poi 之所有 id\n",
        "        targets = poi2ids[id2poi[id_str]]\n",
        "        # 整理 matches 格式 (根據 train + knn 所得) (以空格分割)\n",
        "        preds = set(matches.split())\n",
        "        # 比分數\n",
        "        score = len((targets & preds)) / len((targets | preds))\n",
        "        scores.append(score)\n",
        "    scores = np.array(scores)\n",
        "    return scores.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg9v-MYmqTxZ"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaU66LRsqTxZ"
      },
      "outputs": [],
      "source": [
        "scores = []\n",
        "# 用 poi 去 check 每一個 n_neighors 的 poi，把 poi 相同的 id 加在 matches 這個新增的 column\n",
        "# 此外這裡每次多考慮一個 neighbor，就多算一次分數\n",
        "train[\"matches\"] = \"\"\n",
        "for i in tqdm(range(CFG.n_neighbors)):\n",
        "    # 一次是看某一個i，直的一整大排去賦值\n",
        "    idx = train[CFG.target]==train[f\"near_target_{i}\"]\n",
        "    # idx 裡面是 true 才會 access 到那一個 row\n",
        "    train.loc[idx, \"matches\"] += \" \" + train.loc[idx, f\"near_id_{i}\"]\n",
        "    # 算分數囉～\n",
        "    scores.append(get_score(train))\n",
        "# 把紀錄刪光光\n",
        "train[\"matches\"] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okm0iwZQqTxZ"
      },
      "outputs": [],
      "source": [
        "# 會發現每多考慮一個 candidate 分數就可以提高\n",
        "plt.subplots(figsize=(8, 3), facecolor=\"white\")\n",
        "print(scores)\n",
        "plt.plot(range(CFG.n_neighbors), scores, marker=\"o\")\n",
        "plt.grid()\n",
        "plt.xlabel(\"# of candidates\")\n",
        "plt.ylabel(\"Maximum Score\")\n",
        "plt.ylim([0.6, 1.0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV2kiM0CqTxZ"
      },
      "outputs": [],
      "source": [
        "# 清記憶體垃圾\n",
        "# del train\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T4Dky7pbVYD"
      },
      "outputs": [],
      "source": [
        "train = train.drop('matches', axis=1)\n",
        "train = train.drop('not done', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHe3bRtmqTxZ"
      },
      "outputs": [],
      "source": [
        "train.iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k59Vk9d5Pmx"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8qeYtT4lAkR"
      },
      "outputs": [],
      "source": [
        "if \"google.colab\" in sys.modules:\n",
        "    !pip install Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz6iq7hxqTxa"
      },
      "outputs": [],
      "source": [
        "%load_ext Cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfE2spgkqTxa"
      },
      "outputs": [],
      "source": [
        "%%cython\n",
        "def LCS(str S, str T):\n",
        "    cdef int i, j\n",
        "    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n",
        "    for i in range(len(S)):\n",
        "        for j in range(len(T)):\n",
        "            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n",
        "    return dp[len(S)][len(T)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elWgtrbalAkS"
      },
      "outputs": [],
      "source": [
        "# 利用 Levenshtein、difflib 算出兩字串的各種 features 相異程度數值，並把數值加入到 features 內\n",
        "import Levenshtein\n",
        "import difflib\n",
        "\n",
        "def _add_distance_features(args):\n",
        "    # _, df = args\n",
        "    df = args\n",
        "    columns = ['name', 'address', 'city', 'state',\n",
        "           'zip', 'country', 'url', 'phone', 'categories']\n",
        "\n",
        "    for i in tqdm(range(CFG.n_neighbors)):\n",
        "        for c in columns:\n",
        "            geshs = []\n",
        "            levens = []\n",
        "            jaros = []\n",
        "            lcss = []\n",
        "            sentence_transformer = []\n",
        "            str_enter_count = 0\n",
        "            for str1, str2 in df[[f\"near_{c}_0\", f\"near_{c}_{i}\"]].values.astype(str):\n",
        "                # 一次只會進來一次，所以 for 是為了取值用的～\n",
        "                str_enter_count += str_enter_count\n",
        "                if str_enter_count >= 2:\n",
        "                    print(\"bigger than 2\",\"i = \", i,\"c =\", c)\n",
        "                # 檢查是否為 NaN\n",
        "                if str1==str1 and str2==str2:\n",
        "                    geshs.append(difflib.SequenceMatcher(None, str1, str2).ratio())\n",
        "                    levens.append(Levenshtein.distance(str1, str2))\n",
        "                    jaros.append(Levenshtein.jaro_winkler(str1, str2))\n",
        "                    lcss.append(LCS(str(str1), str(str2)))\n",
        "                else:\n",
        "                    geshs.append(-1)\n",
        "                    levens.append(-1)\n",
        "                    jaros.append(-1)\n",
        "            df[f\"near_{c}_{i}_gesh\"] = geshs\n",
        "            df[f\"near_{c}_{i}_leven\"] = levens\n",
        "            df[f\"near_{c}_{i}_jaro\"] = jaros\n",
        "            df[f\"near_{c}_{i}_lcs\"] = lcss\n",
        "\n",
        "            # 如果是 namen, address, city, state, url, categories 的情況下，再多加一個平均的 feature\n",
        "            if not c in ['country', \"phone\", \"zip\"]:\n",
        "                df[f\"near_{c}_{i}_len\"] = df[f\"near_{c}_{i}\"].astype(str).map(len)\n",
        "                df[f\"near_{c}_{i}_nleven\"] = df[f\"near_{c}_{i}_leven\"] / df[[f\"near_{c}_{i}_len\", f\"near_{c}_0_len\"]].max(axis=1)\n",
        "                df[f\"near_{c}_{i}_nlcsi\"] = df[f\"near_{c}_{i}_lcs\"] / df[f\"near_{c}_{i}_len\"]\n",
        "                df[f\"near_{c}_{i}_nlcs0\"] = df[f\"near_{c}_{i}_lcs\"] / df[f\"near_{c}_0_len\"]\n",
        "            # # 加上 word embedding\n",
        "            # if c in ['name', 'categories']:\n",
        "            #     for str1, str2 in df[[f\"near_{c}_0\", f\"near_{c}_{i}\"]].values.astype(str):\n",
        "            #         if str1==str1 and str2==str2:\n",
        "            #             embeddings_list = model.encode([str1,str2])\n",
        "            #             score = float(util.cos_sim(embeddings_list[0],embeddings_list[1])[0][0])\n",
        "            #             df[f\"near_{c}_{i}_embedder\"] = score\n",
        "            #         else:\n",
        "            #             df[f\"near_{c}_{i}_embedder\"] = -1 \n",
        "    return df\n",
        "\n",
        "# muilty processing\n",
        "def add_distance_features(df):\n",
        "    processes = multiprocessing.cpu_count()\n",
        "    with multiprocessing.Pool(processes=processes) as pool:\n",
        "        dfs = pool.imap_unordered(_add_distance_features, df.groupby('country'))\n",
        "        dfs = tqdm(dfs)\n",
        "        dfs = list(dfs)\n",
        "    df = pd.concat(dfs)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6jKosC8FaI7"
      },
      "outputs": [],
      "source": [
        "len(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpOI_gl2UK3d"
      },
      "outputs": [],
      "source": [
        "training_data = train.sample(frac=0.8, random_state=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CUTUMJswMa3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vHVKTtYUQ0b"
      },
      "outputs": [],
      "source": [
        "len(training_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVGbRAOKRLQw"
      },
      "source": [
        "##stop plz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsoAzC9pqTxa"
      },
      "outputs": [],
      "source": [
        "train = _add_distance_features(training_data)\n",
        "# test = add_distance_features(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3HV5kfs6saZ"
      },
      "source": [
        "# Delete Unusing Columns for avoiding OOM (out of memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHMG8t1UlAkT"
      },
      "outputs": [],
      "source": [
        "# 在上面的 block 我們已經把新的 feature 加入到 dataframe 內了，這裡 features 的目的就是為了移掉其他不重要的 features\n",
        "features = []\n",
        "\n",
        "columns = ['name', 'address', 'city', 'state',\n",
        "       'zip', 'country', 'url', 'phone', 'categories']\n",
        "for i in tqdm(range(CFG.n_neighbors)):\n",
        "    features.append(f\"d_near_{i}\")\n",
        "    for c in columns:        \n",
        "        features += [f\"near_{c}_{i}_gesh\", f\"near_{c}_{i}_jaro\", f\"near_{c}_{i}_lcs\"]\n",
        "        if c in ['country', \"phone\", \"zip\"]:\n",
        "            features += [f\"near_{c}_{i}_leven\"]\n",
        "        else:\n",
        "            features += [f\"near_{c}_{i}_len\", f\"near_{c}_{i}_nleven\", f\"near_{c}_{i}_nlcsi\", f\"near_{c}_{i}_nlcs0\"]\n",
        "\n",
        "for f in features:\n",
        "#     assert f in train.columns\n",
        "    if f not in test.columns:\n",
        "        test[f] = np.nan\n",
        "\n",
        "# print(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKSy9cc3lAkT"
      },
      "outputs": [],
      "source": [
        "# 把剛剛所有資訊放進 train 與 test\n",
        "train = train[features + [CFG.target, \"target\", \"id\"] + [f\"near_id_{i}\" for i in range(CFG.n_neighbors)]]\n",
        "test = test[features + [\"id\"] + [f\"near_id_{i}\" for i in range(CFG.n_neighbors)]]\n",
        "\n",
        "train[features] = train[features].astype(np.float16)\n",
        "test[features] = test[features].astype(np.float16)\n",
        "\n",
        "train[\"target\"] = train[\"target\"].fillna(0)\n",
        "\n",
        "train.reset_index(drop=True, inplace=True)\n",
        "test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "for _ in range(5):\n",
        "    gc.collect()\n",
        "\n",
        "# train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLz0KOD5qTxb"
      },
      "source": [
        "**訓練與測試資料的格式???**\n",
        "- X 包含 \n",
        "    - CFT 的資訊\n",
        "    - n_neighber = 10, 的所有資訊\n",
        "    - n_neighber 跟 CFT.target 相關的所有字串配對分數\n",
        "- y 包含\n",
        "    - 相同 POI 之中，最大的 neighber"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yduGPkpC6y4X"
      },
      "source": [
        "# Split Folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7euwk2q7ly5Y"
      },
      "outputs": [],
      "source": [
        "# kf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
        "# shuffle 應該要是 false? 這樣每個點都才會被平均的當成訓練資料，不會漏掉 > Howard\n",
        "kf = StratifiedKFold(n_splits=CFG.n_splits) \n",
        "for i, (trn_idx, val_idx) in tqdm(enumerate(kf.split(train, train[\"target\"], train[\"target\"]))):\n",
        "    print(i, len(trn_idx), len(val_idx))\n",
        "    train.loc[val_idx, \"fold\"] = i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qknhwIvndmJ_"
      },
      "source": [
        "# Model Learning\n",
        "使用 lightGBM 演算法"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLKg1PSWlAkU"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgbm\n",
        "\n",
        "# n-class 表示分類結果有哪些, n_split 表示 k-fold 做幾次\n",
        "def fit_lgbm(X, y, params=None, es_rounds=20, seed=42, N_SPLITS=5, \n",
        "             n_class=None, model_dir=None, folds=None):\n",
        "    models = []\n",
        "    oof = np.zeros((len(y), n_class), dtype=np.float64)\n",
        "    \n",
        "    # 不同的 split number 就存不同的模型 k-fold cross validation> Howard\n",
        "    for i in tqdm(range(CFG.n_splits)):\n",
        "        print(f\"== fold {i} ==\")\n",
        "        # 把剛剛分完群的拿出來\n",
        "        trn_idx = folds!=i\n",
        "        val_idx = folds==i\n",
        "        # 拿出不同 dataframe\n",
        "        X_train, y_train = X.iloc[trn_idx], y.iloc[trn_idx]\n",
        "        X_valid, y_valid = X.iloc[val_idx], y.iloc[val_idx]\n",
        "        # 不存在就訓練\n",
        "        if model_dir is None:\n",
        "            model = lgbm.LGBMClassifier(**params)\n",
        "            model.fit(\n",
        "                X_train, y_train, \n",
        "                eval_set=[(X_valid, y_valid)],  \n",
        "                early_stopping_rounds=es_rounds, \n",
        "                eval_metric='logloss',  \n",
        "        #       verbose=-1)\n",
        "                verbose=50)\n",
        "        # 存在就 load 出來, 等等跑分數\n",
        "        else:\n",
        "            with open(f'{model_dir}/lgbm_fold{i}.pkl', 'rb') as f:\n",
        "                model = pickle.load(f)\n",
        "        # X_valid 如果是100 筆數據, 且 5個 class,那 pred 的 dimension = (100, 5)\n",
        "        pred = model.predict_proba(X_valid)\n",
        "        oof[val_idx] = pred\n",
        "        models.append(model)\n",
        "        \n",
        "        file = f'lgbm_fold{i}.pkl'\n",
        "        pickle.dump(model, open(file, 'wb'))\n",
        "        print()\n",
        "\n",
        "    cv = (oof.argmax(axis=-1) == y).mean()\n",
        "    print(f\"CV-accuracy: {cv}\")\n",
        "\n",
        "    return oof, models\n",
        "# 直接用 model.predict_proba 跑結果\n",
        "def inference_lgbm(models, feat_df):\n",
        "    pred = np.array([model.predict_proba(feat_df) for model in models])\n",
        "    pred = np.mean(pred, axis=0)\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITp48sX5guF5"
      },
      "source": [
        "## 開始訓練資料"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G77I_9j9mz_G"
      },
      "outputs": [],
      "source": [
        "train[\"target\"].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y22l2kyLlAkV"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'objective': \"logloss\",\n",
        "    'learning_rate': 0.2,\n",
        "    'reg_alpha': 0.1,\n",
        "    'reg_lambda': 0.1,\n",
        "    'random_state': 42,\n",
        "\n",
        "    'max_depth': 7,   \n",
        "    'num_leaves': 35, \n",
        "    'n_estimators': 1000000, \n",
        "    \"colsample_bytree\": 0.9,\n",
        "}\n",
        "\n",
        "oof, models = fit_lgbm(train[features], train[\"target\"].astype(int), \n",
        "                       params=params, n_class=int(train[\"target\"].max()+ 1),\n",
        "                       N_SPLITS=CFG.n_splits, folds=train[\"fold\"].values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW4Xpn-y5uYG"
      },
      "outputs": [],
      "source": [
        "# 根據不同的分組，拿出不同的 model\n",
        "models = [joblib.load(f'lgbm_fold{i}.pkl') for i in range(CFG.n_splits)]\n",
        "# 這裡直接測試test data 並回傳 k-fold 不同模型的預測結果平均\n",
        "pred = inference_lgbm(models, test[features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mFgzGi3qTxc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmVW8BpjlAkW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m_3IY2BqTxc"
      },
      "source": [
        "# Check CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KkH4DiQDOsH"
      },
      "outputs": [],
      "source": [
        "# 每個 id 的周遭 10 個 ids 為誰 (dimension = i * 10)\n",
        "near_ids = train[[f\"near_id_{i}\" for i in range(CFG.n_neighbors)]].values\n",
        "\n",
        "matches = []\n",
        "# 掃過全部的 train row ，得出其中的 id\n",
        "# ps 為所有 id 的預測結果\n",
        "# ids 周遭 10 個 id 為誰\n",
        "for id, ps, ids in tqdm(zip(train[\"id\"], oof, near_ids)):\n",
        "    # 找出最大的那個 class 的值，代表預測的 target 是多少\n",
        "    idx = np.argmax(ps)\n",
        "    matches_string = id\n",
        "\n",
        "    # 如果 idx > 0, 就把小於的也加上去\n",
        "    if idx > 0:\n",
        "      for idx_count in range(1, idx+1):\n",
        "        if ids[idx_count] == ids[idx_count]:\n",
        "            matches_string += \" \"\n",
        "            matches_string += ids[idx_count] \n",
        "    matches.append(matches_string)\n",
        "train[\"matches\"] = matches\n",
        "print(f\"CV: {get_score(train):.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHFNkcnglAkW"
      },
      "outputs": [],
      "source": [
        "# # 每個 id 的周遭 10 個 ids 為誰 (dimension = i * 10)\n",
        "# near_ids = train[[f\"near_id_{i}\" for i in range(CFG.n_neighbors)]].values\n",
        "\n",
        "# matches = []\n",
        "# # 掃過全部的 train row ，得出其中的 id\n",
        "# # ps 為所有 id 的預測結果\n",
        "# # ids 周遭 10 個 id 為誰\n",
        "# for id, ps, ids in tqdm(zip(train[\"id\"], oof, near_ids)):\n",
        "#     # 找出最大的那個 class 的值，代表預測的 target 是多少\n",
        "#     idx = np.argmax(ps)\n",
        "#     # 如果 idx 不是自己，且也不是 Nan, 就 append 自己 跟 idx 進去\n",
        "#     if idx > 0 and ids[idx]==ids[idx]:\n",
        "#         matches.append(id + \" \" + ids[idx])\n",
        "#     # 否則只 append 自己\n",
        "#     else:\n",
        "#         matches.append(id)\n",
        "# train[\"matches\"] = matches\n",
        "# print(f\"CV: {get_score(train):.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG4Mgfj_qTxc"
      },
      "outputs": [],
      "source": [
        "# # test one more time\n",
        "# near_ids = test[[f\"near_id_{i}\" for i in range(CFG.n_neighbors)]].values\n",
        "\n",
        "# matches = []\n",
        "# for id, ps, ids in tqdm(zip(test[\"id\"], pred, near_ids)):\n",
        "#     idx = np.argmax(ps)\n",
        "#     if idx > 0 and ids[idx]==ids[idx]:\n",
        "#         matches.append(id + \" \" + ids[idx])\n",
        "#     else:\n",
        "#         matches.append(id)\n",
        "# test[\"matches\"] = matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUYwysGxqTxc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydzBOFIlqTxc"
      },
      "source": [
        "# Check Feature Importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7YqLSXPlAkX"
      },
      "outputs": [],
      "source": [
        "def plot_importances(models):\n",
        "    importance_df = pd.DataFrame(models[0].feature_importances_, \n",
        "                                 index=features, \n",
        "                                 columns=['importance'])\\\n",
        "                        .sort_values(\"importance\", ascending=False)\n",
        "\n",
        "    plt.subplots(figsize=(len(features) // 4, 5))\n",
        "    plt.bar(importance_df.index, importance_df.importance)\n",
        "    plt.grid()\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.ylabel(\"importance\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_importances(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HLehVKtuqTxc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BocZtBoXqTxd"
      },
      "source": [
        "# Simple Post-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UffaiHGelAkW"
      },
      "outputs": [],
      "source": [
        "def postprocess(df):\n",
        "    id2match = dict(zip(df[\"id\"].values, df[\"matches\"].str.split()))\n",
        "\n",
        "    for match in tqdm(df[\"matches\"]):\n",
        "        match = match.split()\n",
        "        if len(match) == 1:        \n",
        "            continue\n",
        "        \n",
        "        base = match[0]\n",
        "        for m in match[1:]:\n",
        "            if not base in id2match[m]:\n",
        "                id2match[m].append(base)\n",
        "    df[\"matches\"] = df[\"id\"].map(id2match).map(\" \".join)\n",
        "    return df \n",
        "\n",
        "#train = postprocess(train)\n",
        "test = postprocess(test)\n",
        "# print(f\"CV: {get_score(train):.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LHL-qF7UqTxd"
      },
      "outputs": [],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGRb86B-qTxd"
      },
      "source": [
        "# Submit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6wCPJF-qqTxd"
      },
      "outputs": [],
      "source": [
        "ssub = pd.read_csv(\"submission.csv\")\n",
        "ssub = ssub.drop(columns=\"matches\")\n",
        "ssub = ssub.merge(test[[\"id\", \"matches\"]], on=\"id\")\n",
        "ssub.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "ssub.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "32B9_o2YtSl7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# embedder test"
      ],
      "metadata": {
        "id": "xryMogysHXg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "embedding_name = model.encode(train['name'])\n",
        "embedding_categories = model.encode(train['categories'])"
      ],
      "metadata": {
        "id": "tSL3B1NRHWUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(embedding_name)):\n",
        "    embedding_name[i] = str(embedding_name[i])\n",
        "    embedding_categories[i] = str(embedding_categories[i])\n",
        "\n",
        "train['embedding_name'] = embedding_name\n",
        "train['embedding_categories'] = embedding_categories"
      ],
      "metadata": {
        "id": "LguGT0QfHrRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_name_1 = [\"[1, 1, 1]\", \"[2, 3, 1]\",\"[20, 30, 10]\"]\n",
        "embedding_name_2 = [\"[0, 0, 1]\", \"[1, 0, 0]\",\"[20, 30, 10]\"]\n",
        "df = pd.DataFrame(zip(embedding_name_1,embedding_name_2), columns = ['name1', 'name2'])"
      ],
      "metadata": {
        "id": "6nztGjdMHtN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys \n",
        "sys.path.append(\"../input/sentence-transformer/sentence-transformers-2.2.0\") \n",
        "import sentence_transformers"
      ],
      "metadata": {
        "id": "E_b0v9S4HwSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import util\n",
        "import ast"
      ],
      "metadata": {
        "id": "pZspPdZeHzN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_similarity(df_column1, df_column2):\n",
        "    list_1 = list(map(ast.literal_eval, df_column1))\n",
        "    list_1 = [[float(long_num) for long_num in long_list] for long_list in list_1]\n",
        "    list_2 = list(map(ast.literal_eval, df_column2))\n",
        "    list_2 = [[float(long_num) for long_num in long_list] for long_list in list_2]\n",
        "    score = util.cos_sim(list_1,list_2)\n",
        "    print(type(list_1[0]))\n",
        "    print(list_1, list_2)\n",
        "    one_to_one_score = [ float(score[i][i]) for i in range(len(score))]\n",
        "    return one_to_one_score"
      ],
      "metadata": {
        "id": "iv-K8wzKH2IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = word_similarity(df['name1'], df['name2'])"
      ],
      "metadata": {
        "id": "z7RPzG9RH53d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(score), score)\n"
      ],
      "metadata": {
        "id": "ua2Bl0AnH8TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(score)"
      ],
      "metadata": {
        "id": "NrObMwguH-aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first = ast.literal_eval(train[f'embedding_{c}_{i}'])\n",
        "second = ast.literal_eval(train[f'embedding_{c}_{i}'])\n",
        "df[f\"near_{c}_{i}_embedder\"] = score"
      ],
      "metadata": {
        "id": "TpEVgFCHIAqh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "baseline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}