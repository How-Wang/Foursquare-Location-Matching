{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"參考 link :https://www.kaggle.com/code/ryotayoshinobu/foursquare-lightgbm-baseline\nThis notebook shows how to solve the problem as a multi-class classification by finding candidate points based on geographic location.<br>\nSimilarity as a string, such as edit distance and LCS (Longest Common Subsequence), was used for the features of the candidate points.<br>\n<br>\nInference is made on test data only, but the code for training is left commented out.<br>\n<br>\nIn addition, making the matches bidirectional as a post-processing step improved the score by about 1%.<br>\n<br>","metadata":{"id":"RfFhJlZbqTxJ"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport os\nimport gc\nimport random\nfrom glob import glob\nfrom sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\nimport warnings\nimport seaborn as sns\nimport pickle\nimport json\nimport re\nimport time\nimport sys\nfrom requests import get\nimport multiprocessing\nimport joblib\n# from sentence_transformers import SentenceTransformer, util\n# model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n\n# 宣告一個專門放參數的 class\nclass CFG: # configuration\n    seed = 46\n    target = \"point_of_interest\"\n    n_neighbors = 10\n    n_splits = 3\n\n    expID = \"\"\n    if \"google.colab\" in sys.modules:\n        expID = get(\"http://172.28.0.2:9000/api/sessions\").json()[0][\"name\"].split(\".\")[0]\n\nrandom.seed(CFG.seed)\nos.environ[\"PYTHONHASHSEED\"] = str(CFG.seed)\nnp.random.seed(CFG.seed)\n\nplt.rcParams[\"font.size\"] = 13\nwarnings.filterwarnings('ignore')\n\n# %cd /content/drive/MyDrive/kaggle/foursquare-location-matching/{CFG.expID}","metadata":{"id":"H5QntWoelAkH","outputId":"b589c3ec-e04b-4c92-d3dc-90483fa25a73","execution":{"iopub.status.busy":"2022-06-06T09:26:48.310286Z","iopub.execute_input":"2022-06-06T09:26:48.311239Z","iopub.status.idle":"2022-06-06T09:26:49.927481Z","shell.execute_reply.started":"2022-06-06T09:26:48.311070Z","shell.execute_reply":"2022-06-06T09:26:49.926272Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# train = pd.read_csv(\"./train.csv\")\ntest = pd.read_csv(\"../input/foursquare-location-matching/test.csv\")\ntest[CFG.target] = \"TEST\"\n\n# test.head()","metadata":{"id":"wz7JepVilAkN","outputId":"0b89b29d-46ad-4846-e221-ea981eb87c1c","execution":{"iopub.status.busy":"2022-06-06T09:26:49.929158Z","iopub.execute_input":"2022-06-06T09:26:49.930270Z","iopub.status.idle":"2022-06-06T09:26:49.957850Z","shell.execute_reply.started":"2022-06-06T09:26:49.930204Z","shell.execute_reply":"2022-06-06T09:26:49.956513Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Divide Train Data into about 600K×2","metadata":{"id":"lO9c6tIe3j4B"}},{"cell_type":"code","source":"# kf = GroupKFold(n_splits=2)\n# for i, (trn_idx, val_idx) in enumerate(kf.split(train, train[CFG.target], train[CFG.target])):\n#     train.loc[val_idx, \"set\"] = i\n# train[\"set\"].value_counts()","metadata":{"id":"U6PcXKsn3pcK","outputId":"4af2e505-19ac-42fc-9047-d266d8327ac2","execution":{"iopub.status.busy":"2022-06-06T09:26:49.960464Z","iopub.execute_input":"2022-06-06T09:26:49.961141Z","iopub.status.idle":"2022-06-06T09:26:49.966448Z","shell.execute_reply.started":"2022-06-06T09:26:49.961084Z","shell.execute_reply":"2022-06-06T09:26:49.965366Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# train.head()","metadata":{"id":"Ewfee-Td3pYM","outputId":"25537cde-61c6-4d40-9392-dbde90776d48","execution":{"iopub.status.busy":"2022-06-06T09:26:49.969732Z","iopub.execute_input":"2022-06-06T09:26:49.970800Z","iopub.status.idle":"2022-06-06T09:26:49.978572Z","shell.execute_reply.started":"2022-06-06T09:26:49.970693Z","shell.execute_reply":"2022-06-06T09:26:49.977437Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# 資料代入 / 插補 / 填補 Data Imputation","metadata":{"id":"9yJIRkRD3jr-"}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n# 把每一個地點都加上 另外一個最近地點的資訊\ndef add_neighbor_features(df):\n    dfs = []\n    # 需要 id 不然不沒辦法做後方 matches 的 set、沒辦法算分數\n    columns = ['id', 'name', 'address', 'city', 'state',\n           'zip', 'country', 'url', 'phone', 'categories']\n    for c in columns:\n        if c != \"id\":\n            df[c] = df[c].astype(str).str.lower()\n            \n    # 把相同國家的 row 放進同一個 dataframe 裡面\n    for country, country_df in tqdm(df.groupby(\"country\")):\n        country_df = country_df.reset_index(drop=True)\n        # n_neighbors 表示要參考附近的幾筆資料才決定屬於哪一群\n        k = min(len(country_df), CFG.n_neighbors)\n        knn = KNeighborsRegressor(n_neighbors=k, \n                                  metric='haversine', n_jobs=-1)\n        \n        # 使用相同的\"國家群\"進行訓練\n        # 同個國家的資料當成一群進行訓練\n        knn.fit(country_df[['latitude','longitude']], country_df.index)\n        \n        # 得出分群後的資料，回傳每個點與最近的幾個點的距離與 indices\n        dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n        # 將 n_neighbors 各點的資料呈現在 country_df 上\n        targets = country_df[CFG.target].values\n        for i in range(k):\n            country_df[f\"d_near_{i}\"] = dists[:, i]\n            country_df[f\"near_target_{i}\"] = targets[nears[:, i]]\n            for c in columns:\n                country_df[f\"near_{c}_{i}\"] = country_df[c].values[nears[:, i]]\n        # 若整個國家的資料點數量小於預設所需的 k ，則將多出的部分填補為 nan\n        for i in range(k, CFG.n_neighbors):\n            country_df[f\"d_near_{i}\"] = np.nan\n            country_df[f\"near_target_{i}\"] = np.nan\n            for c in columns:\n                country_df[f\"near_{c}_{i}\"] = np.nan\n\n        dfs.append(country_df)\n    df = pd.concat(dfs).reset_index(drop=True)\n    return df\n","metadata":{"id":"rsyHcuGDlAkO","execution":{"iopub.status.busy":"2022-06-06T09:26:49.980835Z","iopub.execute_input":"2022-06-06T09:26:49.981534Z","iopub.status.idle":"2022-06-06T09:26:50.160834Z","shell.execute_reply.started":"2022-06-06T09:26:49.981487Z","shell.execute_reply":"2022-06-06T09:26:50.159411Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"test = add_neighbor_features(test)","metadata":{"id":"SzYSOaJDqTxW","outputId":"e9bd6e7f-e137-4038-f14c-390530b90604","execution":{"iopub.status.busy":"2022-06-06T09:26:50.163069Z","iopub.execute_input":"2022-06-06T09:26:50.163907Z","iopub.status.idle":"2022-06-06T09:26:51.021417Z","shell.execute_reply.started":"2022-06-06T09:26:50.163851Z","shell.execute_reply":"2022-06-06T09:26:51.020166Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be57f9c83e514468867773a9f84ace7a"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Two-way Hash\n用雜湊表來將正確答案產出","metadata":{"id":"gtM0grrKqTxY"}},{"cell_type":"code","source":"# https://www.kaggle.com/code/columbia2131/foursquare-iou-metrics\n\n## 將input dataframe 轉成 dict，再拿出 id 與 poi 用 dict 回傳\ndef get_id2poi(input_df: pd.DataFrame) -> dict:\n    return dict(zip(input_df['id'], input_df['point_of_interest']))\n\n## 將input dataframe 轉成 dict，再以 poi 為 index、取出\"屬於此 poi\"的 id set 用 dict 回傳\ndef get_poi2ids(input_df: pd.DataFrame) -> dict:\n    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n\n# id2poi = get_id2poi(train)\n# poi2ids = get_poi2ids(train)","metadata":{"id":"N6wMZkeUqTxY","execution":{"iopub.status.busy":"2022-06-06T09:26:51.023753Z","iopub.execute_input":"2022-06-06T09:26:51.024092Z","iopub.status.idle":"2022-06-06T09:26:51.032267Z","shell.execute_reply.started":"2022-06-06T09:26:51.024048Z","shell.execute_reply":"2022-06-06T09:26:51.031455Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## 比對\n利用 KNN 的結果做出預測，再與正確答案做比對","metadata":{"id":"y2CF7M8cqTxY"}},{"cell_type":"code","source":"## 已有 matches\ndef get_score(input_df: pd.DataFrame):\n    scores = []\n    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n        # 找出與該 id 相同 poi 之所有 id\n        targets = poi2ids[id2poi[id_str]]\n        # 整理 matches 格式 (根據 train + knn 所得) (以空格分割)\n        preds = set(matches.split())\n        # 比分數\n        score = len((targets & preds)) / len((targets | preds))\n        scores.append(score)\n    scores = np.array(scores)\n    return scores.mean()","metadata":{"id":"9dDr4VvpqTxZ","execution":{"iopub.status.busy":"2022-06-06T09:26:51.033756Z","iopub.execute_input":"2022-06-06T09:26:51.034543Z","iopub.status.idle":"2022-06-06T09:26:51.053696Z","shell.execute_reply.started":"2022-06-06T09:26:51.034499Z","shell.execute_reply":"2022-06-06T09:26:51.052702Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# 清記憶體垃圾\n# del train\ngc.collect()","metadata":{"id":"LV2kiM0CqTxZ","outputId":"f139e7b9-b49e-4da5-bad8-33bfcffafde3","execution":{"iopub.status.busy":"2022-06-06T09:26:51.055014Z","iopub.execute_input":"2022-06-06T09:26:51.055642Z","iopub.status.idle":"2022-06-06T09:26:51.229701Z","shell.execute_reply.started":"2022-06-06T09:26:51.055608Z","shell.execute_reply":"2022-06-06T09:26:51.228145Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"182"},"metadata":{}}]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{"id":"6k59Vk9d5Pmx"}},{"cell_type":"code","source":"if \"google.colab\" in sys.modules:\n    !pip install Levenshtein","metadata":{"id":"R8qeYtT4lAkR","outputId":"086fe0dd-c319-47d2-e021-b4fe089339a8","execution":{"iopub.status.busy":"2022-06-06T09:26:51.233527Z","iopub.execute_input":"2022-06-06T09:26:51.234555Z","iopub.status.idle":"2022-06-06T09:26:51.246322Z","shell.execute_reply.started":"2022-06-06T09:26:51.234506Z","shell.execute_reply":"2022-06-06T09:26:51.245438Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"%load_ext Cython","metadata":{"id":"Xz6iq7hxqTxa","execution":{"iopub.status.busy":"2022-06-06T09:26:51.247680Z","iopub.execute_input":"2022-06-06T09:26:51.248520Z","iopub.status.idle":"2022-06-06T09:26:52.639400Z","shell.execute_reply.started":"2022-06-06T09:26:51.248470Z","shell.execute_reply":"2022-06-06T09:26:52.638419Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"%%cython\ndef LCS(str S, str T):\n    cdef int i, j\n    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n    for i in range(len(S)):\n        for j in range(len(T)):\n            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n    return dp[len(S)][len(T)]","metadata":{"id":"LfE2spgkqTxa","execution":{"iopub.status.busy":"2022-06-06T09:26:52.640539Z","iopub.execute_input":"2022-06-06T09:26:52.640814Z","iopub.status.idle":"2022-06-06T09:26:54.337629Z","shell.execute_reply.started":"2022-06-06T09:26:52.640775Z","shell.execute_reply":"2022-06-06T09:26:54.336306Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# 利用 Levenshtein、difflib 算出兩字串的各種 features 相異程度數值，並把數值加入到 features 內\nimport Levenshtein\nimport difflib\n\ndef _add_distance_features(args):\n    # _, df = args\n    df = args\n    columns = ['name', 'address', 'city', 'state',\n           'zip', 'country', 'url', 'phone', 'categories']\n\n    for i in tqdm(range(CFG.n_neighbors)):\n        for c in columns:\n            geshs = []\n            levens = []\n            jaros = []\n            lcss = []\n            sentence_transformer = []\n            str_enter_count = 0\n            for str1, str2 in df[[f\"near_{c}_0\", f\"near_{c}_{i}\"]].values.astype(str):\n                # 一次只會進來一次，所以 for 是為了取值用的～\n                str_enter_count += str_enter_count\n                if str_enter_count >= 2:\n                    print(\"bigger than 2\",\"i = \", i,\"c =\", c)\n                # 檢查是否為 NaN\n                if str1==str1 and str2==str2:\n                    geshs.append(difflib.SequenceMatcher(None, str1, str2).ratio())\n                    levens.append(Levenshtein.distance(str1, str2))\n                    jaros.append(Levenshtein.jaro_winkler(str1, str2))\n                    lcss.append(LCS(str(str1), str(str2)))\n                else:\n                    geshs.append(-1)\n                    levens.append(-1)\n                    jaros.append(-1)\n            df[f\"near_{c}_{i}_gesh\"] = geshs\n            df[f\"near_{c}_{i}_leven\"] = levens\n            df[f\"near_{c}_{i}_jaro\"] = jaros\n            df[f\"near_{c}_{i}_lcs\"] = lcss\n\n            # 如果是 name, address, city, state, url, categories 的情況下，再多加一個平均的 feature\n            if not c in ['country', \"phone\", \"zip\"]:\n                df[f\"near_{c}_{i}_len\"] = df[f\"near_{c}_{i}\"].astype(str).map(len)\n                df[f\"near_{c}_{i}_nleven\"] = df[f\"near_{c}_{i}_leven\"] / df[[f\"near_{c}_{i}_len\", f\"near_{c}_0_len\"]].max(axis=1)\n                df[f\"near_{c}_{i}_nlcsi\"] = df[f\"near_{c}_{i}_lcs\"] / df[f\"near_{c}_{i}_len\"]\n                df[f\"near_{c}_{i}_nlcs0\"] = df[f\"near_{c}_{i}_lcs\"] / df[f\"near_{c}_0_len\"]\n    return df\n\n# muilty processing\ndef add_distance_features(df):\n    processes = multiprocessing.cpu_count()\n    with multiprocessing.Pool(processes=processes) as pool:\n        dfs = pool.imap_unordered(_add_distance_features, df.groupby('country'))\n        dfs = tqdm(dfs)\n        dfs = list(dfs)\n    df = pd.concat(dfs)\n    return df","metadata":{"id":"elWgtrbalAkS","execution":{"iopub.status.busy":"2022-06-06T09:26:54.339639Z","iopub.execute_input":"2022-06-06T09:26:54.339924Z","iopub.status.idle":"2022-06-06T09:26:54.364271Z","shell.execute_reply.started":"2022-06-06T09:26:54.339879Z","shell.execute_reply":"2022-06-06T09:26:54.363571Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## stop plz","metadata":{"id":"KVGbRAOKRLQw"}},{"cell_type":"code","source":"test = _add_distance_features(test)","metadata":{"id":"tsoAzC9pqTxa","outputId":"85f9bc35-2ad4-40a7-bdac-81019573c477","execution":{"iopub.status.busy":"2022-06-06T09:26:54.365989Z","iopub.execute_input":"2022-06-06T09:26:54.366274Z","iopub.status.idle":"2022-06-06T09:26:54.893142Z","shell.execute_reply.started":"2022-06-06T09:26:54.366238Z","shell.execute_reply":"2022-06-06T09:26:54.892152Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9088fae73f2d4f1dbc24f142e33ea932"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Delete Unused Columns for avoiding OOM (out of memory)","metadata":{"id":"Q3HV5kfs6saZ"}},{"cell_type":"code","source":"# 在上面的 block 我們已經把轉換過的 feature 加入到 dataframe 內了，這裡 features 的目的就是為了移掉其他不重要的 features\nfeatures = []\n\ncolumns = ['name', 'address', 'city', 'state',\n       'zip', 'country', 'url', 'phone', 'categories']\nfor i in tqdm(range(CFG.n_neighbors)):\n    features.append(f\"d_near_{i}\")\n    for c in columns:        \n        features += [f\"near_{c}_{i}_gesh\", f\"near_{c}_{i}_jaro\", f\"near_{c}_{i}_lcs\"]\n        if c in ['country', \"phone\", \"zip\"]:\n            features += [f\"near_{c}_{i}_leven\"]\n        else:\n            features += [f\"near_{c}_{i}_len\", f\"near_{c}_{i}_nleven\", f\"near_{c}_{i}_nlcsi\", f\"near_{c}_{i}_nlcs0\"]\n\nfor f in features:\n#     assert f in test.columns\n    if f not in test.columns:\n        test[f] = np.nan\n\n# print(features)","metadata":{"id":"GHMG8t1UlAkT","outputId":"7398d41c-2e00-4985-a368-ac52649a37ef","execution":{"iopub.status.busy":"2022-06-06T09:26:54.894621Z","iopub.execute_input":"2022-06-06T09:26:54.894874Z","iopub.status.idle":"2022-06-06T09:26:54.936978Z","shell.execute_reply.started":"2022-06-06T09:26:54.894843Z","shell.execute_reply":"2022-06-06T09:26:54.935587Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19b75e493c774752b8ffb3e30f8a23fe"}},"metadata":{}}]},{"cell_type":"code","source":"test = test[features + [\"id\"] + [f\"near_id_{i}\" for i in range(CFG.n_neighbors)]]\n\ntest[features] = test[features].astype(np.float16)\n\ntest.reset_index(drop=True, inplace=True)\n\nfor _ in range(5):\n    gc.collect()\n","metadata":{"id":"aKSy9cc3lAkT","execution":{"iopub.status.busy":"2022-06-06T09:26:54.938534Z","iopub.execute_input":"2022-06-06T09:26:54.938756Z","iopub.status.idle":"2022-06-06T09:26:55.795413Z","shell.execute_reply.started":"2022-06-06T09:26:54.938728Z","shell.execute_reply":"2022-06-06T09:26:55.794693Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**訓練與測試資料的格式**\n- X 包含 \n    - CFT 的資訊\n    - n_neighber = 10, 的所有資訊\n    - n_neighber 跟 CFT.target 相關的所有字串配對分數\n- y 包含\n    - 相同 POI 之中，最大的 neighber","metadata":{"id":"zLz0KOD5qTxb"}},{"cell_type":"markdown","source":"# Model Running\n使用 lightGBM 演算法","metadata":{"id":"qknhwIvndmJ_"}},{"cell_type":"code","source":"# 直接用 model.predict_proba 跑結果\ndef inference_lgbm(models, feat_df):\n    pred = np.array([model.predict_proba(feat_df) for model in models])\n    pred = np.mean(pred, axis=0)\n    return pred","metadata":{"execution":{"iopub.status.busy":"2022-06-06T09:26:55.796573Z","iopub.execute_input":"2022-06-06T09:26:55.797552Z","iopub.status.idle":"2022-06-06T09:26:55.805419Z","shell.execute_reply.started":"2022-06-06T09:26:55.797500Z","shell.execute_reply":"2022-06-06T09:26:55.803840Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# 根據不同的分組，拿出不同的 model\nmodels = [joblib.load(f'../input/continue-target/continue_target/lgbm_fold{i}.pkl') for i in range(CFG.n_splits)]\n# 這裡直接測試test data 並回傳 k-fold 不同模型的預測結果平均\npred_con = inference_lgbm(models, test[features])","metadata":{"id":"GW4Xpn-y5uYG","execution":{"iopub.status.busy":"2022-06-06T09:26:55.807327Z","iopub.execute_input":"2022-06-06T09:26:55.807718Z","iopub.status.idle":"2022-06-06T09:26:55.873081Z","shell.execute_reply.started":"2022-06-06T09:26:55.807675Z","shell.execute_reply":"2022-06-06T09:26:55.871575Z"},"trusted":true},"execution_count":18,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/3142538192.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 根據不同的分組，拿出不同的 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'../input/continue-target/continue_target/lgbm_fold{i}.pkl'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# 這裡直接測試test data 並回傳 k-fold 不同模型的預測結果平均\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpred_con\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_lgbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_33/3142538192.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 根據不同的分組，拿出不同的 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'../input/continue-target/continue_target/lgbm_fold{i}.pkl'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# 這裡直接測試test data 並回傳 k-fold 不同模型的預測結果平均\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpred_con\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_lgbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/continue-target/continue_target/lgbm_fold0.pkl'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/continue-target/continue_target/lgbm_fold0.pkl'","output_type":"error"}]},{"cell_type":"code","source":"# 根據不同的分組，拿出不同的 model\nmodels = [joblib.load(f'../input/80percent/80percent/lgbm_fold{i}.pkl') for i in range(CFG.n_splits)]\n# 這裡直接測試test data 並回傳 k-fold 不同模型的預測結果平均\npred_far = inference_lgbm(models, test[features])","metadata":{"execution":{"iopub.status.busy":"2022-06-06T09:26:55.874494Z","iopub.status.idle":"2022-06-06T09:26:55.875654Z","shell.execute_reply.started":"2022-06-06T09:26:55.875271Z","shell.execute_reply":"2022-06-06T09:26:55.875310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check CV","metadata":{"id":"5m_3IY2BqTxc"}},{"cell_type":"code","source":"# 每個 id 的周遭 10 個 ids 為誰 (dimension = i * 10)\nnear_ids = test[[f\"near_id_{i}\" for i in range(CFG.n_neighbors)]].values\n\nmatches = []\n# 掃過全部的 test row ，得出其中的 id\n# ps 為所有 id 的預測結果\n# ids 周遭 10 個 id 為誰\nfor id, ps_con, ps_far, ids in tqdm(zip(test[\"id\"], pred_con, pred_far, near_ids)):\n    # 找出最大的那個 class 的值，代表預測的 target 是多少\n    idx_con = np.argmax(ps_con)\n    idx_far = np.argmax(ps_far)\n    matches_string = id\n    if idx_con > idx_far:\n        for idx_count in range(1, idx_con +1):\n            if ids[idx_count] == ids[idx_count]:\n                matches_string += \" \"\n                matches_string += ids[idx_count]\n    elif idx_con < idx_far:\n        for idx_count in range(1, idx_con +1):\n            if ids[idx_count] == ids[idx_count]:\n                matches_string += \" \"\n                matches_string += ids[idx_count]\n        if ids[idx_far] == ids[idx_far]:\n            matches_string += \" \"\n            matches_string += ids[idx_far]\n    else: # idx_con == idx_far:\n        if idx_con > 0:\n          for idx_count in range(1, idx_con +1):\n            if ids[idx_count] == ids[idx_count]:\n                matches_string += \" \"\n                matches_string += ids[idx_count] \n    matches.append(matches_string)\ntest[\"matches\"] = matches\n# print(f\"CV: {get_score(test):.6f}\")","metadata":{"id":"yHFNkcnglAkW","execution":{"iopub.status.busy":"2022-06-06T09:26:55.877602Z","iopub.status.idle":"2022-06-06T09:26:55.878369Z","shell.execute_reply.started":"2022-06-06T09:26:55.878047Z","shell.execute_reply":"2022-06-06T09:26:55.878078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple Post-Processing","metadata":{"id":"BocZtBoXqTxd"}},{"cell_type":"code","source":"def postprocess(df):\n    id2match = dict(zip(df[\"id\"].values, df[\"matches\"].str.split()))\n\n    for match in tqdm(df[\"matches\"]):\n        match = match.split()\n        if len(match) == 1:        \n            continue\n        \n        base = match[0]\n        for m in match[1:]:\n            if not base in id2match[m]:\n                id2match[m].append(base)\n    df[\"matches\"] = df[\"id\"].map(id2match).map(\" \".join)\n    return df \n\n#test = postprocess(test)\ntest = postprocess(test)\n# print(f\"CV: {get_score(test):.6f}\")","metadata":{"id":"UffaiHGelAkW","execution":{"iopub.status.busy":"2022-06-06T09:26:55.879672Z","iopub.status.idle":"2022-06-06T09:26:55.880657Z","shell.execute_reply.started":"2022-06-06T09:26:55.880428Z","shell.execute_reply":"2022-06-06T09:26:55.880457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"id":"LHL-qF7UqTxd","execution":{"iopub.status.busy":"2022-06-06T09:26:55.881855Z","iopub.status.idle":"2022-06-06T09:26:55.882224Z","shell.execute_reply.started":"2022-06-06T09:26:55.882036Z","shell.execute_reply":"2022-06-06T09:26:55.882060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit","metadata":{"id":"kGRb86B-qTxd"}},{"cell_type":"code","source":"ssub = pd.read_csv(\"../input/foursquare-location-matching/sample_submission.csv\")\nssub = ssub.drop(columns=\"matches\")\nssub = ssub.merge(test[[\"id\", \"matches\"]], on=\"id\")\nssub.to_csv(\"submission.csv\", index=False)\n\nssub.head()","metadata":{"id":"6wCPJF-qqTxd","execution":{"iopub.status.busy":"2022-06-06T09:26:55.883723Z","iopub.status.idle":"2022-06-06T09:26:55.884071Z","shell.execute_reply.started":"2022-06-06T09:26:55.883890Z","shell.execute_reply":"2022-06-06T09:26:55.883912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"32B9_o2YtSl7"},"execution_count":null,"outputs":[]}]}